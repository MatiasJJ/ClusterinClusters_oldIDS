{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as scp\n",
    "import nltk\n",
    "import imageio\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import pairwise_distances \n",
    "\n",
    "from timeit import default_timer as timer\n",
    "%matplotlib inline\n",
    "\n",
    "import json as js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reverse PCA\n",
    "- polish this stuff\n",
    "- make python scrips\n",
    "- make readme nice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "### Stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4x20x10000 stuctures with 19-30 atoms in a cluster\n",
    "    - coordinates\n",
    "    - dipole moment\n",
    "    - energy\n",
    "- Goals\n",
    "    - End user\n",
    "        - Configuration Sampling \n",
    "            - Find one structure with the lowest energy\n",
    "    - Eliminate redundant structures\n",
    "    - Find descriptors\n",
    "        - reduce dimensions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this you should provide:  \n",
    "- the **path** to your XTB-data\n",
    "- the **name** for your data\n",
    "\n",
    "> The name will be used to name the folder where data produced by this code is put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTB_path = input(\"Where your XTB-data is?\")\n",
    "theName = input(\"Your data will be in:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(XTB_path),XTB_path)\n",
    "print(type(theName),theName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $XTB_path\n",
    "!echo $theName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook user specifies the path of where the XTB data is and what is the name of the data (for the folder)\n",
    "\n",
    "In python scripts the user is promted about where the XTB data is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data must have a `.csv`-file that contains:  \n",
    "Header row: ``` Filename,LogPath,XYZPath,Dipole,Energy ```\n",
    "\n",
    "\n",
    "| Filename | LogPath | XYZPath | Dipole| Energy |\n",
    "|------|------|------|------|------|\n",
    "| Name of the cluster | path to .log-file | path to .xyz-file | Dipole moment value | Energy value |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we either require the .log-file or remove it from the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the csv-file \n",
    "> Instructions how to make the .csv-file from the actual data using the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run these cells with ```shift+enter```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_DF(csvfile):\n",
    "    \"Reads a xyz-file and initializes the dataframe.\"\n",
    "    \n",
    "    filescsv = pd.read_csv(csvfile)                    # Read tthe .csv-file to a DataFrame\n",
    "    xyz_temp = filescsv.iloc[0,2]                      # Get one .xyz-file as a reference\n",
    "    \n",
    "    xyz_test = read_xyz(xyz_temp,flatten=False)        # Read coordinates to a DF (no flattening)\n",
    "    \n",
    "    #filenames = filescsv.iloc[:,0]                    # Get the names for files as a Series (This one not used here)\n",
    "    n_atoms = xyz_test.index.size                      # Number of atoms from the reference .xyz-file\n",
    "    atomnames = np.array(xyz_test.index)               # Names of all the atoms as ndarray\n",
    "    \n",
    "    list_atomnames = []\n",
    "    for i,j in enumerate(atomnames):                   # Give numbers for the atoms\n",
    "        list_atomnames.append(\"{}_{}\".format(j,i))\n",
    "    arr_atomnames = np.array(list_atomnames)           # Enumerated atomnames as an ndarray\n",
    "    \n",
    "    col_names = makeMultiIndexs(arr_atomnames)         # Make column names (multi-index)\n",
    "\n",
    "    arr_clusters,filenames = init_xyz_array(filescsv,n_atoms) # Read the actual coordinate data\n",
    "    \n",
    "    coord_df = pd.DataFrame(arr_clusters,index=filenames,columns=col_names) # Save the data in nice DF-fornat\n",
    "    \n",
    "    \n",
    "    return coord_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xyz_array(filecsv, n_atoms):\n",
    "    \"\"\"Reads .xyz-file paths from .csv-file and returns \n",
    "    an array with all xyz-coords in rows, and the filenames for DF indexes\"\"\"\n",
    "    \n",
    "    arr_clusters = np.zeros((1,3*n_atoms+2))             # cols of coord array should be of shape [clusters,atoms*[x,y,z]]\n",
    "    filenames = np.array(filecsv.Filename)               # Get the names of the clusters from the .csv-file\n",
    "\n",
    "    for index, row in filecsv.iterrows():                # Iterate through clusters and save their attributes\n",
    "        dipole = row.Dipole                              # find the dipole value\n",
    "        energy = row.Energy                              # find the energy value\n",
    "        metarr = np.array([energy,dipole])               # put dipole and energy to an array\n",
    "        path = row.XYZPath                               # find the path for .xyz-file\n",
    "        xyzs = read_xyz(path)                            # Read coordinates to an ndarray with flatten=True (default)\n",
    "\n",
    "        #if index == None:                               # print stuff for debugging purposes of this function\n",
    "        #    print(\"Filename:\",filename)\n",
    "        #    print(\"metarr:\",metarr)\n",
    "        #    print(\"shape:\",metarr.shape)\n",
    "        #    print(\"type:\",type(metarr))\n",
    "        #    print(\"path:\",path)\n",
    "        #    print(\"coordshape:\",xyzs.shape)\n",
    "        #    print(\"coords:\",xyzs)\n",
    "        \n",
    "        arrr = np.array(list(metarr)+list(xyzs))         # concatenate one cluster info\n",
    "        arr_clusters = np.vstack((arr_clusters,arrr))    # concatenate clusters to one array\n",
    "    return arr_clusters[1:,:], filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xyz(xyzfile,flatten=True):\n",
    "    \"\"\"\n",
    "    usage: read_xyz(xyzfile,flatten(Default=True))\n",
    "    \n",
    "    Reads a xyz-file and returns the coordinates as DF\n",
    "    or flattened to a Numpy-array\n",
    "    \"\"\"\n",
    "    xyz_temp = pd.read_csv(xyzfile, \\\n",
    "                       sep='\\s+', \\\n",
    "                       skiprows=2, \\\n",
    "                       header=None, \\\n",
    "                       index_col=0, \\\n",
    "                       names=['x','y','z']\\\n",
    "                      ).sort_index(ascending=False)       # Reads coordinates from an xyz-file\n",
    "    \n",
    "    atoms3 = xyz_temp.shape[0]*3                          # multiply n of atoms with 3 dimensions\n",
    "    \n",
    "    if flatten==True:\n",
    "        xyz_flat = xyz_temp.values.flatten().reshape(1,atoms3)#.astype(np.float)\n",
    "        return xyz_flat[0]                                # returns coordinates as ndarray of shape [1,atoms*3]\n",
    "    else:\n",
    "        return xyz_temp                                   # returns coordinates as a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeMultiIndexs(arr):\n",
    "    \"\"\"\n",
    "    Gets names of atoms as array and makes a pandas multi-indexing structure\n",
    "    with x-, y- and z- columns for each atom.\n",
    "    \"\"\"\n",
    "    top_arr = np.empty((arr.size * 3 +2)).astype(str)\n",
    "    top_arr[0] = \"Properties\"\n",
    "    top_arr[1] = \"Properties\"\n",
    "    top_arr[2::3] = arr\n",
    "    top_arr[3::3] = arr\n",
    "    top_arr[4::3] = arr\n",
    "    butt_arr = np.empty((arr.size * 3 +2)).astype(str)\n",
    "    butt_arr[0] = \"Energy\"\n",
    "    butt_arr[1] = \"Dipole\"\n",
    "    butt_arr[2::3] = \"x\"\n",
    "    butt_arr[3::3] = \"y\"\n",
    "    butt_arr[4::3] = \"z\"\n",
    "    col_arr = np.vstack((top_arr,butt_arr))\n",
    "    #col_arr.transpose()\n",
    "    col_arr_tuples = list(zip(*col_arr))\n",
    "    col_names = pd.MultiIndex.from_tuples(col_arr_tuples, names=[\"Atom\",\"Coord\"])\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df = init_DF(\"Data2/Data_Collection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from multi-index to one index\n",
    "clusters_df.columns = ['_'.join(col) for col in clusters_df.columns.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration / EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking at the correlation among our variables. The correlation betwwen two variables is a measure of the linear dependecy among them:\n",
    "$$ Cor(X,Y)=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}$$\n",
    "In order to apply a linear model to predict the energy wrt the other variables, is important to check that there is a small correlation  between the variables we are using as predictors, since we will predict the response by an approprate linear combination of the predictors. In our data, there is no particoularly high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = clusters_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11,9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations among data are not really high, hence we can consider all of hem as independent variables.   \n",
    "Having a look on the distributions of the energy variable, it is asymmetric, with a tail on the right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "sns.pairplot(clusters_df,  kind=\"reg\",vars=['Properties_Energy','Properties_Dipole'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy and dipole don't have significant correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df['Properties_Energy'].hist()\n",
    "plt.title(\"Energy\")\n",
    "plt.axvline(np.mean(clusters_df['Properties_Energy']), color='k', linestyle='dashed', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between min and max energies is very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot( y=\"Properties_Energy\", kind=\"box\", data=clusters_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm in ABCluster tries to make the energies as small as possible hich can be seen here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy=clusters_df['Properties_Energy'] # %1 should take only the decimals but does it?\n",
    "coord=clusters_df.drop(['Properties_Energy','Properties_Dipole'],1) # dipoles and energy out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "In statistics we define as \"supervised learning\" the set of models that are ment to evaluate an output $y$ (answer variable) as a function of a set of known indipendent variables taken as input $(X_1,...,X_k)$ .   \n",
    "Furthermore, if we assume that $Y$ has a known probability law, we use parametric models. When $Y$ is a continuous variable, the mainly used model is the regression, that is a method that allows to estimate the conditional average of the answer $ E (Y | X_1 = x_1, ..., X_k = x_k) $ or its transformed as a linear combination of the explanatory $\\eta=X '\\beta $.\n",
    "We have a simple linear regression, when using a model of the type:\n",
    "$$ Y_i = x_i '\\beta + \\varepsilon_i $$ with $i = 1, ..., n $ number of observations, such that $ n> k $. Equivalently, in compact form:\n",
    "$$ Y = X \\beta + \\varepsilon $$ where $ Y_ {nx1} = (Y_1, ..., Y_n) '$ is the vector of n v.c. of independent outputs, $ X_ {nx (k + 1)} $ is the matrix of the drawing, which together with the vector of coefficients $ \\beta = (\\beta_0, ..., \\beta_k) '$ constitutes the deterministic component of the model, while $ \\varepsilon = (\\varepsilon_1, ..., \\varepsilon_n) '$ is the erratic component, where $ \\varepsilon_i \\sim N (0, \\sigma ^ 2 _ {\\varepsilon}) $ for each $ i $ are unknown errors with zero mean and constant variance.\n",
    "\n",
    "We can implement a linear regression model to predict the energy ($y$) of our cluster with rrespect to the coordinates variables ($X$). As we saw above, our response takes all values around -40.9, what makes the difference is the centesimal portion, hence we will focus our attention on predicting this part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the dataset in train and test\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(coord,energy,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression model on train set\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.coeff\n",
    "r_sq = model.score(x_train, y_train)\n",
    "r_sq #pretty bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict from the test\n",
    "y_pred = model.predict(x_test)\n",
    "pred_test = pd.DataFrame({'Actual': y_test.values.flatten(), 'Predicted': y_pred.flatten()})\n",
    "pred_test.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the dataset in train and test\n",
    "\n",
    "coord_train,coord_test,energy_train,energy_test = train_test_split(coord,energy,test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression model on train set\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(coord_train, energy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.coeff\n",
    "r_sq = model.score(coord_train, energy_train)\n",
    "r_sq #pretty bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression works poorly which means that the conformation in general doesn't correlate with the energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict from the test\n",
    "energy_pred = model.predict(coord_test)\n",
    "predictions = pd.DataFrame({'Actual': energy_test.values.flatten(), 'Predicted': energy_pred.flatten()})\n",
    "predictions.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection alternative\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\n",
    "\n",
    "estimator = SVR(kernel=\"linear\")\n",
    "selector = RFE(estimator, 5, step=1)\n",
    "selector = selector.fit(coord_train, energy_train)\n",
    "#print(selector.support_) \n",
    "print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But what is this then selecting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 2 hydrogens the $R^2$ value is smaller than with all the atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oxigen O8 seems the most significant variable in our model\n",
    "model1 = LinearRegression().fit(x_train.loc[:,('H_17_x','H_17_y','H_17_z','H_18_x','H_18_y','H_18_z')], y_train) #select the most significant variables...\n",
    "print('R2 simple reg',model1.score(x_train.loc[:,('H_17_x','H_17_y','H_17_z','H_18_x','H_18_y','H_18_z')], y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the other atoms than hydrogens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_train.iloc[:5,:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omit hydrogens\n",
    "model2 = LinearRegression().fit(coord_train.iloc[:,:33], energy_train) #select the most significant variables...\n",
    "print('R2 simple reg',model2.score(coord_train.iloc[:,:33], energy_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "The PCA is a tool used to reduce the dimensionality of the data.\n",
    "It tries to preserve as much information as possible.  \n",
    "These two goals in PCA are pursued by means of a transformation of the original variables into new variables, called Principal Components (PCs).  \n",
    "They consists of a linear combination of the original variables.\n",
    "PCs are uncorrelated and arranged in order of decreasing variance, so that the first PCs account for most of the variation in the sample.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming we want to reduce the number of our original p variables to $k<p$ variables, the PCA method can be formalized as follows:  \n",
    "• 1st PC: determine the coeffcients of the linear combination\n",
    "$$Z_{1j} = a_1^T X = \\sum_{i=0}^p a_{1i} X_i $$ \n",
    "that maximize $ Var(Z_1) = a_1^T \\Sigma a_1 $ under the constraint $ a_1^T a_1 = 1 $   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The formula $ Var(Z_1) = a_1^T \\Sigma a_1 $ looks weird"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• 2nd PC: determine coeffcients of the linear combination\n",
    "$$ Z_{2j} = a_2^T X = \\sum_{i=0}^p a_{2i} X_i $$\n",
    "that maximize $ Var(Z_2) = a_2^T \\Sigma a_2$ under the constraint $ a_2^T a_2=1$ and $Cov(Z_1,Z_2)=0$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• proceed in a similar fashion for all other components.   \n",
    "\n",
    "• Final output: $Z_1, \\dots ,Z_p $such that $Var (Z1) > Var (Z2) > \\dots > Var (Zp)$ and $cov(Zj ; Zk ) = 0$ for $j \\neq k$  \n",
    "\n",
    "We will apply PCA to our coordinates variable, in order to select new variables, that explain at least 80% of the variability of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.8,svd_solver='full')   # from docstring: this should include PCs so that 80% of variance is explained\n",
    "PCs_coord = pca.fit_transform(coord)            # fit_transform ehkä normalisoi\n",
    "PCs_df = pd.DataFrame(data = PCs_coord, columns=[\"PC{}\".format(i) for i in range(PCs_coord.shape[1])])\n",
    "print('Explained variance :', np.sum(pca.explained_variance_ratio_)*100, '%')    # It seems that it is 80% :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2D graph of first two PCs\n",
    "print('Explained variance PC1 :', pca.explained_variance_ratio_[0]*100, '%')\n",
    "print('Explained variance PC2 :', pca.explained_variance_ratio_[1]*100, '%')\n",
    "\n",
    "sns.scatterplot(x=PCs_df['PC1'], y=PCs_df['PC2'], hue=energy,  alpha=0.3)\n",
    "plt.title(\"2 first PCs of PCA\")\n",
    "plt.ylabel('PC2')\n",
    "plt.xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here $\\uparrow$ could be some clustering seen but the representation is poor (cartesian coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means / Clustering\n",
    "In order to reduce the variety of our observations we are clustering our data, by using a k-mean algorith.  \n",
    "This algorithm is a non-hierarchical methods of clustering, i.e. we assume the number $ k $ of groups is assumed to be fixed.\n",
    "Algorithm introduced by MacQueen (1967): each statistical unit is assigned to the cluster whose centroid (i.e. vector of means) is the closest one. The metric used to measure the distance among groups is typically the Euclidean one.\n",
    "\n",
    "1. Initial partition into K clusters, (possibly randomly generated)\n",
    "2. For each of the K clusters, compute the cluster centroid.\n",
    "3. Assign each observation to the cluster whose centroid is closest\n",
    "4. Recompute centroids for all clusters\n",
    "5. Repeat 3. - 4. until reaching a maximum number of iterations or when it is not possible to redistribute observations\n",
    "\n",
    "In order to chose the optimal number of groups, a good criteria is to find a balance between low within-cluster variation $WSS$ and number of groups.  \n",
    "We start by clustering our original dataset, a clustering of the PCs dtaset is also shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Energy and dipole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_ED = KMeans(n_clusters=4)  #we should choose the number of clusters\n",
    "kmeans_ED.fit(clusters_df) \n",
    "ED_kmeans_res = kmeans_ED.predict(clusters_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(clusters_df['Properties_Energy'], clusters_df['Properties_Dipole'], c=ED_kmeans_res,s=50, cmap='viridis', alpha=0.5)\n",
    "centers_ED = kmeans_ED.cluster_centers_\n",
    "plt.scatter(centers_ED[:, 0], centers_ED[:, 1], c='red', s=100, alpha=0.5)\n",
    "plt.xlabel('Energy')\n",
    "plt.ylabel('Dipole')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCsE_df = pd.concat([PCs_df, clusters_df[['Properties_Energy']]], axis = 1)  \n",
    "#finalDf = pd.concat([finalDf, clusters_df[['Properties_Dipole']]], axis = 1) # leave dipoles out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coordinates as PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering using PCS as variables\n",
    "kmeans_PCs = KMeans(n_clusters=4)\n",
    "kmeans_PCs.fit(PCs_df)\n",
    "PCs_kmeans = kmeans_PCs.predict(PCs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCs_df['PC1'], PCs_df['PC2'], c=PCs_kmeans,s=3, cmap='viridis', alpha=0.2)\n",
    "centers_PCs = kmeans_PCs.cluster_centers_\n",
    "plt.scatter(centers_PCs[:, 0], centers_PCs[:, 1], c='black', s=200, alpha=0.5)\n",
    "plt.ylabel('PC2')\n",
    "plt.xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coordinates as PCs and Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering using PCS+E as variables\n",
    "kmeans_PCE = KMeans(n_clusters=23)\n",
    "kmeans_PCE.fit(PCsE_df)\n",
    "PCE_kmeans = kmeans_PCE.predict(PCsE_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCsE_df['PC1'], PCsE_df['PC2'], c=PCE_kmeans,s=3, cmap='viridis', alpha=0.2)\n",
    "centers_PCE = kmeans_PCE.cluster_centers_\n",
    "plt.scatter(centers_PCE[:, 0], centers_PCE[:, 1], c='black', s=200, alpha=0.5)\n",
    "plt.ylabel('PC2')\n",
    "plt.xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for optimal k = amount of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = np.arange(3,40)\n",
    "ch_values = np.empty(k_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in k_values:\n",
    "    kmeans_model = KMeans(n_clusters=k,random_state=1)\n",
    "    kmeans_model.fit(PCsE_df)\n",
    "    labels = kmeans_model.labels_\n",
    "    ch_values[k-k_values[0]] = metrics.calinski_harabasz_score(PCsE_df,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best = k_values[np.where(ch_values == ch_values.max())[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering using PCS+E as variables with best k-value\n",
    "kbest_PCE = KMeans(n_clusters=k_best)\n",
    "kbest_PCE.fit(PCsE_df)\n",
    "PCE_kbest = kbest_PCE.predict(PCsE_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCsE_df['PC1'], PCsE_df['PC2'], c=PCE_kbest,s=3, cmap='viridis', alpha=0.2)\n",
    "centers_kbest = kbest_PCE.cluster_centers_\n",
    "plt.scatter(centers_kbest[:, 0], centers_kbest[:, 1], c='black', s=200, alpha=0.5)\n",
    "plt.ylabel('PC2')\n",
    "plt.xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_PCE = kmeans_PCE.labels_\n",
    "metrics.calinski_harabasz_score(PCsE_df,labels_PCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_PCE = TSNE(n_components=2).fit_transform(PCsE_df)    # make t-SNE magic happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centers_tSNE = TSNE(n_components=2).fit_transform(centers_PCE)  # Same for centers which doesn't work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_PCE = tsne.fit_transform(PCsE_df)\n",
    "centers_tSNE = tsne.fit_transform(centers_PCE)\n",
    "```\n",
    "> Does this the same as upper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_PCE_cluster = pd.concat((pd.DataFrame(tsne_PCE,columns=['tsne1','tsne2']), pd.DataFrame(PCE_kmeans,columns=['groupID'])), axis = 1)  \n",
    "# combine t-SNE info with the info of the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_tSNE = tsne_PCE_cluster.groupby(by='groupID').mean()\n",
    "# calculate the centers as means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!say \"The code is ready\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(tsne_PCE[:,0],tsne_PCE[:,1], c=PCE_kmeans,s=3, cmap='viridis', alpha=0.3)\n",
    "\n",
    "plt.scatter(centers_tSNE.iloc[:, 0], centers_tSNE.iloc[:, 1], c='black', s=200, alpha=0.5)\n",
    "plt.title(\"Clusters' Cluster\")\n",
    "plt.ylabel('t-SNE component 2')\n",
    "plt.xlabel('t-SNE component 1')\n",
    "plt.savefig(\"broccoli_centers.png\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try to get PCA reversed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "X = sklearn.datasets.load_iris().data \n",
    "print(X[:5])\n",
    "\n",
    "mu = np.mean(X, axis=0) \n",
    "print(mu)\n",
    "\n",
    "peeceeaa = PCA()\n",
    "peeceeaa.fit(X)\n",
    "\n",
    "nComp = 2 \n",
    "Xhat = np.dot(peeceeaa.transform(X)[:,:nComp], peeceeaa.components_[:nComp,:]) \n",
    "print(Xhat[:5])\n",
    "\n",
    "Xhat += mu \n",
    "print(np.around(Xhat[:5],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The stuff forslecting the structures\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "import sklearn.datasets, sklearn.decomposition \n",
    " \n",
    "X = sklearn.datasets.load_iris().data \n",
    "mu = np.mean(X, axis=0) \n",
    " \n",
    "pca = sklearn.decomposition.PCA() \n",
    "pca.fit(X) \n",
    " \n",
    "nComp = 2 \n",
    "Xhat = np.dot(pca.transform(X)[:,:nComp], pca.components_[:nComp,:]) \n",
    "Xhat += mu \n",
    " \n",
    "print(Xhat[0,])\n",
    "\n",
    "to determin the optimal k inmkmeans algo, we can choose the indicator that mazimize this index (Kalinkski and harabatz)\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look for the nice amount of clusters\n",
    "from the chem point of view there should be 23 different clusters (permutations of molecules) so that would be nice to try that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#clustering using PCS and energies as variables and t-SNE for visualization\n",
    "\n",
    "kmeans_PCs.fit(DF_embedded)\n",
    "y_kmeans = kmeans_PCs.predict(DF_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables in this notebook\n",
    "\n",
    "> still under construction\n",
    "\n",
    "| Variable             | Usage                    | Data type   |\n",
    "| :------------------- | :----------------------- | :-----------|\n",
    "| `clusters_df`        | Whole data               | DF          |\n",
    "| `energy`             | energies col             | Series      |\n",
    "| `coord`              | coordinate data          | DF          |\n",
    "| `coord_train`        | x for training           | array       |\n",
    "| `coord_test`         | x for testing            | array       |\n",
    "| `energy_train`       | y for training           | array       |\n",
    "| `energy_test`        | y for testing            | array       |\n",
    "| `model`              | Linear Reg               | ***         |\n",
    "| `r_sq`               | $ R^2 $-score            | float       |\n",
    "| `energy_pred`        | predicted energies       | array       |\n",
    "| `predictions`        | pred comparison          | DF          |\n",
    "| `pca`                | PCA analysis obejct      | pca         |\n",
    "| `PCs_coord`          | PCs of coordinates       | array?      |\n",
    "| `PCs_df`             | PCs of coordinates       | DF          |\n",
    "| `PCsE_df`            | PCs and energies         | DF          |\n",
    "| `kmeans_ED`          | K-means object E+dipole  | kmeans      |\n",
    "| `ED_kmeans_res`      | K-means prediction       | array       |\n",
    "| `centers_ED`         | E+dipol centers for plot | array       |\n",
    "| `kmeans_PCs`         | K-means object PCA       | kmeans      |\n",
    "| `PCs_kmeans`         | K-means pred from PCs    | array       |\n",
    "| `centers_PCs`        | PCs centers for plot     | array       |\n",
    "| `kmeans_PCE`         | K-means object PCs+E     | kmeans      |\n",
    "| `PCE_kmeans`         | K-means pred from PCs+E  | array       |\n",
    "| `centers_PCE`        | E+PCA centers for plot   | array       |\n",
    "| `DF_embedded`        | Stuff to t-SNE           | ???         |\n",
    "| `centers_tSNE`       | E+PCA centers for t-SNE  | array       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We propably want to develop a functionality to actually copy wanted data into a nice folder\n",
    "- eg. all clusters belonging to a cluster into a same folder\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCE_kmeans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
